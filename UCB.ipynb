{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import numpy as np\n",
    "import sys\n",
    "\n",
    "if \"../\" not in sys.path:\n",
    "    sys.path.append(\"../\") \n",
    "\n",
    "from lib.envs.bandit import BanditEnv\n",
    "from lib.simulation import Experiment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Policy interface\n",
    "class Policy:\n",
    "    #num_actions: (int) Number of arms [indexed by 0 ... num_actions-1]\n",
    "    def __init__(self, num_actions):\n",
    "        self.num_actions = num_actions\n",
    "    \n",
    "    def act(self):\n",
    "        pass\n",
    "        \n",
    "    def feedback(self, action, reward):\n",
    "        pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Greedy policy\n",
    "class Greedy(Policy):\n",
    "    def __init__(self, num_actions):\n",
    "        Policy.__init__(self, num_actions)\n",
    "        self.name = \"Greedy\"\n",
    "        self.total_rewards = np.zeros(num_actions, dtype = np.longdouble)\n",
    "        self.total_counts = np.zeros(num_actions, dtype = np.longdouble)\n",
    "    \n",
    "    def act(self):\n",
    "        current_averages = np.divide(self.total_rewards, self.total_counts, where = self.total_counts > 0)\n",
    "        current_averages[self.total_counts <= 0] = 0.5      #Correctly handles Bernoulli rewards; over-estimates otherwise\n",
    "        current_action = np.argmax(current_averages)\n",
    "        return current_action\n",
    "        \n",
    "    def feedback(self, action, reward):\n",
    "        self.total_rewards[action] += reward\n",
    "        self.total_counts[action] += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Epsilon Greedy policy\n",
    "class EpsilonGreedy(Greedy):\n",
    "    def __init__(self, num_actions, epsilon):\n",
    "        Greedy.__init__(self, num_actions)\n",
    "        if (epsilon is None or epsilon < 0 or epsilon > 1):\n",
    "            print(\"EpsilonGreedy: Invalid value of epsilon\", flush = True)\n",
    "            sys.exit(0)\n",
    "            \n",
    "        self.epsilon = epsilon\n",
    "        self.name = \"Epsilon Greedy\"\n",
    "    \n",
    "    def act(self):\n",
    "        choice = None\n",
    "        if self.epsilon == 0:\n",
    "            choice = 0\n",
    "        elif self.epsilon == 1:\n",
    "            choice = 1\n",
    "        else:\n",
    "            choice = np.random.binomial(1, self.epsilon)\n",
    "            \n",
    "        if choice == 1:\n",
    "            return np.random.choice(self.num_actions)\n",
    "        else:\n",
    "            current_averages = np.divide(self.total_rewards, self.total_counts, where = self.total_counts > 0)\n",
    "            current_averages[self.total_counts <= 0] = 0.5  #Correctly handles Bernoulli rewards; over-estimates otherwise\n",
    "            current_action = np.argmax(current_averages)\n",
    "            return current_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "#UCB policy\n",
    "class UCB(Greedy):\n",
    "    def __init__(self, num_actions):\n",
    "        Greedy.__init__(self, num_actions)\n",
    "        self.name = \"UCB\"\n",
    "        self.round = 0\n",
    "        \n",
    "    def act(self):\n",
    "        current_action = None\n",
    "        self.round += 1\n",
    "        if self.round <= self.num_actions:\n",
    "            \"\"\"The first k rounds, where k is the number of arms/actions, play each arm/action once\"\"\"\n",
    "            current_action = 0\n",
    "        else:\n",
    "            \"\"\"At round t, play the arms with maximum average and exploration bonus\"\"\"\n",
    "            current_action = 0\n",
    "        return current_action"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "evaluation_seed = 1239\n",
    "num_actions = 10\n",
    "trials = 10000\n",
    "distribution = \"bernoulli\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Distribution: bernoulli [0.5061565  0.74836123 0.53065236 0.37446716 0.88168477 0.83849367\n",
      " 0.3951277  0.13217982 0.44509856 0.03459039]\n",
      "Optimal arm: 4\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "module 'lib.plotting' has no attribute 'plot_arm_rewards'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-19-b7182cb0d74b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0magent\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mUCB\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_actions\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mexperiment\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mExperiment\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0magent\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 4\u001b[0;31m \u001b[0mexperiment\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun_bandit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtrials\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m~/Reinforcement-Learning-Algorithms/lib/simulation.py\u001b[0m in \u001b[0;36mrun_bandit\u001b[0;34m(self, max_number_of_trials, display_frequency)\u001b[0m\n\u001b[1;32m     78\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdistribution\u001b[0m \u001b[0;34m!=\u001b[0m \u001b[0;34m\"normal\"\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 80\u001b[0;31m             \u001b[0mplotting\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mplot_arm_rewards\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0menv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreward_parameters\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     81\u001b[0m         \u001b[0;31m#else:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     82\u001b[0m             \u001b[0;31m#plotting.plot_arm_rewards(self.env.reward_parameters[0])\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: module 'lib.plotting' has no attribute 'plot_arm_rewards'"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<Figure size 720x360 with 0 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "env = BanditEnv(num_actions, distribution, evaluation_seed)\n",
    "agent = UCB(num_actions)\n",
    "experiment = Experiment(env, agent)\n",
    "experiment.run_bandit(trials)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
